{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 20160913\n",
      "Saved to file czce.txt\n",
      "added 20160914\n",
      "Saved to file czce.txt\n",
      "added 20160919\n",
      "added 20160920\n",
      "added 20160921\n",
      "Saved to file czce.txt\n",
      "Saved to file czce.txt\n",
      "added 20160913\n",
      "Saved to file shfe.txt\n",
      "added 20160914\n",
      "error 20160915\n",
      "error 20160916\n",
      "added 20160919\n",
      "added 20160920\n",
      "added 20160921\n",
      "Saved to file shfe.txt\n",
      "Saved to file shfe.txt\n",
      "added 20160913\n",
      "Saved to file dce.txt\n",
      "added 20160914\n",
      "added 20160915\n",
      "added 20160916\n",
      "Saved to file dce.txt\n",
      "added 20160919\n",
      "added 20160920\n",
      "added 20160921\n",
      "Saved to file dce.txt\n",
      "Saved to file dce.txt\n",
      "added:  ('20160913', 79387, 165853)\n",
      "Saved to file cffex.txt\n",
      "added:  ('20160914', 76526, 165359)\n",
      "error  20160915\n",
      "error  20160916\n",
      "added:  ('20160919', 55500, 156292)\n",
      "added:  ('20160920', 44597, 159836)\n",
      "added:  ('20160921', 49804, 162021)\n",
      "Saved to file cffex.txt\n",
      "Saved to file cffex.txt\n",
      "2016-09-13 6,765,656,012\n",
      "Saved to file szse_main.txt\n",
      "2016-09-14 6,202,817,898\n",
      "error 2016-09-15\n",
      "error 2016-09-16\n",
      "2016-09-19 5,843,585,362\n",
      "2016-09-20 5,835,569,072\n",
      "2016-09-21 5,991,643,346\n",
      "Saved to file szse_main.txt\n",
      "Saved to file szse_main.txt\n",
      "2016-09-13 6,549,378,819\n",
      "Saved to file szse_sme.txt\n",
      "2016-09-14 6,194,425,005\n",
      "error 2016-09-15\n",
      "error 2016-09-16\n",
      "2016-09-19 5,832,441,652\n",
      "2016-09-20 6,088,958,203\n",
      "2016-09-21 5,915,376,621\n",
      "Saved to file szse_sme.txt\n",
      "Saved to file szse_sme.txt\n"
     ]
    }
   ],
   "source": [
    "from grab import Grab\n",
    "from datetime import timedelta, date, datetime\n",
    "import time\n",
    "from random import randint\n",
    "import requests\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: generator with business days\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "            \n",
    "            \n",
    "def save_to_file(data_save, filename):\n",
    "    \"\"\"save data to filename\"\"\"\n",
    "    \n",
    "    with open(\"{}.txt\".format(filename), \"a\") as f:\n",
    "        for raw in data_save:\n",
    "            for obj in raw:\n",
    "                # exclude not numerical symbols\n",
    "                num = str(obj).replace(',', '').replace('-', '')\n",
    "                if obj == raw[0]:\n",
    "                    # convert to excel datetime format\n",
    "                    dt = datetime.strptime(num, \"%Y%m%d\")\n",
    "                    num = datetime.strftime(dt, \"%Y-%m-%d\")\n",
    "                f.write(str(num))\n",
    "\n",
    "                if obj != raw[-1]:\n",
    "                    # do not need write ',' after last symbol\n",
    "                    f.write(', ')\n",
    "            f.write('\\n')\n",
    "    print(\"Saved to file {}\".format(f.name))\n",
    "\n",
    "\n",
    "def czce(dates):\n",
    "    \"\"\"parse data from czce\n",
    "    different link after 20151006\n",
    "    \"\"\"\n",
    "    \n",
    "    _links = []\n",
    "    # old link format\n",
    "    # url = 'http://english.czce.com.cn/enportal/exchange/marketdata/{0}/datadaily/{1}.htm'\n",
    "    # new link format from 20151008\n",
    "    url = 'http://english.czce.com.cn/enportal/DFSStaticFiles/Future/{0}/{1}/EnglishFutureDataDaily.htm'\n",
    "    for single_date in dates:\n",
    "        _links.append(url.format(single_date.strftime(\"%Y\"), single_date.strftime(\"%Y%m%d\")))\n",
    "        \n",
    "    g = Grab()\n",
    "    _data = []\n",
    "    for indx, l in enumerate(_links):\n",
    "        try:\n",
    "            g.go(l)\n",
    "            for i in g.doc.select('//*[@id=\"senfe\"]/tr'):\n",
    "                if 'Total' in i.text():\n",
    "                    # old format\n",
    "                    # _data.append((l[-12:-4], i.text().split(' ')[1], i.text().split(' ')[2]))\n",
    "                    # print('added', l[-12:-4])\n",
    "                    # new format\n",
    "                    _data.append((l[-35:-27], i.text().split(' ')[1], i.text().split(' ')[2]))\n",
    "                    print('added', l[-35:-27])\n",
    "            \n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(_data, 'czce')\n",
    "                _data.clear()\n",
    "\n",
    "                time.sleep(randint(0, 3))  # try to avoid ban\n",
    "        except:\n",
    "            print('error', l[-35:-27])\n",
    "            continue\n",
    "    save_to_file(_data, 'czce')\n",
    "\n",
    "\n",
    "def shfe(dates):\n",
    "    \"\"\"parse shfe data\"\"\"\n",
    "    \n",
    "    _links = []\n",
    "    url = 'http://www.shfe.com.cn/data/dailydata/kx/kx{0}.dat'\n",
    "    for single_date in dates:\n",
    "        _links.append(url.format(single_date.strftime(\"%Y%m%d\")))\n",
    "    \n",
    "    _data = []\n",
    "    for indx, l in enumerate(_links):\n",
    "        try:\n",
    "            r = requests.get(l).json()\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "            _data.append((l[-12:-4], r['o_curinstrument'][-1]['VOLUME'],\n",
    "                         r['o_curinstrument'][-1]['OPENINTEREST']))\n",
    "            print('added', l[-12:-4])\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(_data, 'shfe')\n",
    "                _data.clear()\n",
    "        except:\n",
    "            print('error', l[-12:-4])\n",
    "            continue\n",
    "    save_to_file(_data, 'shfe')\n",
    "\n",
    "    \n",
    "def dce(dates):\n",
    "    \"\"\"grab data from dce\"\"\"\n",
    "    \n",
    "    g = Grab()\n",
    "    url = 'http://www.dce.com.cn/PublicWeb/MainServlet'\n",
    "\n",
    "    res = []\n",
    "    for indx, date in enumerate(dates):\n",
    "        try:\n",
    "            # post request\n",
    "            cur_date = int(date.strftime(\"%Y%m%d\"))\n",
    "            _data = {'action': 'Pu00231_result',\n",
    "                'Pu00231_Input.trade_date': cur_date,\n",
    "                'Pu00231_Input.variety': 'all',\n",
    "                'Pu00231_Input.trade_type': 0,\n",
    "                'Submit': 'Go'}\n",
    "            g.go(url, post=_data)\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "            # find 'total' element and grab its Volume and OI\n",
    "            for i in g.doc.select('//body/table/tr/td/*/tr'):\n",
    "                if 'Total' in i.text():\n",
    "                    d = i.text().split(' ')\n",
    "                    res.append((cur_date, d[1], d[2]))\n",
    "            print('added', cur_date)\n",
    "            # save all, every 3rd element\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(res, 'dce')\n",
    "                res.clear()\n",
    "        except:\n",
    "            print('error', cur_date)\n",
    "            continue\n",
    "    save_to_file(res, 'dce')\n",
    "\n",
    "\n",
    "def cffex(dates):\n",
    "    \"\"\"grab data from cffex\"\"\"\n",
    "    \n",
    "    url = 'http://www.cffex.com.cn/fzjy/mrhq/{0}/{1}/index.xml'\n",
    "    _links = []\n",
    "\n",
    "    for date in dates:\n",
    "        _links.append(url.format(date.strftime(\"%Y%m\"), date.strftime(\"%d\")))\n",
    "    \n",
    "    g = Grab()\n",
    "    _data = []\n",
    "\n",
    "    for indx, l in enumerate(_links):\n",
    "        try:\n",
    "            g.go(l)\n",
    "            tree = g.doc.build_xml_tree()\n",
    "\n",
    "            daily_vol = []\n",
    "            daily_oi = []\n",
    "\n",
    "            for i in tree:\n",
    "                for j in i.iter():\n",
    "                    if j.tag == 'volume':\n",
    "                        # print(j.tag, ' = ', j.text)\n",
    "                        vol = j.text\n",
    "                        daily_vol.append(int(vol.split('.')[0]))\n",
    "                    elif j.tag == 'openinterest':\n",
    "                        # print(j.tag, ' = ', j.text)\n",
    "                        oi = j.text\n",
    "                        daily_oi.append(int(oi.split('.')[0]))\n",
    "                        # print(sum(daily_vol), sum(daily_oi))\n",
    "            _data.append((l[-19:-13] + l[-12:-10], sum(daily_vol), sum(daily_oi)))\n",
    "            daily_vol.clear()\n",
    "            daily_oi.clear()\n",
    "            print('added: ', _data[-1])\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(_data, 'cffex')\n",
    "                _data.clear()\n",
    "        except:\n",
    "            print('error ', l[-19:-13] + l[-12:-10])\n",
    "            continue\n",
    "    save_to_file(_data, 'cffex')\n",
    "\n",
    "    \n",
    "def szse_main(dates):\n",
    "    \"\"\"parse data from szse main board\"\"\"\n",
    "    \n",
    "    g = Grab()\n",
    "    url = 'http://www.szse.cn/szseWeb/FrontController.szse?randnum=0.06895916919770584'\n",
    "\n",
    "    res = []\n",
    "    for indx, date in enumerate(dates):\n",
    "        try:\n",
    "            # post request\n",
    "            cur_date = date.strftime(\"%Y-%m-%d\")\n",
    "            post_data = {'ACTIONID':7,\n",
    "                        'AJAX':'AJAX-TRUE',\n",
    "                        'CATALOGID':1849,\n",
    "                        'txtQueryDate':cur_date,\n",
    "                        'TABKEY':'tab2'}\n",
    "            g.go(url, post=post_data)\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "            # find 'shares traded' element and grab its Volume\n",
    "            volume = []\n",
    "            for tr in g.doc.select('//table[@class=\"cls-data-table\"]/tr'):\n",
    "                if 'Shares' in tr.text():\n",
    "                    for td in tr.select('td'):\n",
    "                        volume.append(td.text())\n",
    "            print(cur_date,volume[1])\n",
    "            res.append((cur_date, volume[1]))\n",
    "            # save all, every 3rd element\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(res, 'szse_main')\n",
    "                res.clear()\n",
    "        except:\n",
    "            print('error', cur_date)\n",
    "            continue\n",
    "    save_to_file(res, 'szse_main')\n",
    "\n",
    "    \n",
    "def szse_sme(dates):\n",
    "    \"\"\"grab data from SZSE SME Board\"\"\"\n",
    "    \n",
    "    g = Grab()\n",
    "    url = 'http://www.szse.cn/szseWeb/FrontController.szse?randnum=0.06895916919770584'\n",
    "\n",
    "    res = []\n",
    "    for indx, date in enumerate(dates):\n",
    "        try:\n",
    "            # post request\n",
    "            cur_date = date.strftime(\"%Y-%m-%d\")\n",
    "            post_data = {'ACTIONID':7,\n",
    "                        'AJAX':'AJAX-TRUE',\n",
    "                        'CATALOGID':1849,\n",
    "                        'txtQueryDate':cur_date,\n",
    "                        'TABKEY':'tab3'}\n",
    "            g.go(url, post=post_data)\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "            # find 'shares traded' element and grab its Volume\n",
    "            volume = []\n",
    "            for tr in g.doc.select('//table[@class=\"cls-data-table\"]/tr'):\n",
    "                if 'Shares' in tr.text():\n",
    "                    for td in tr.select('td'):\n",
    "                        volume.append(td.text())\n",
    "            print(cur_date,volume[1])\n",
    "            res.append((cur_date, volume[1]))\n",
    "            # save all, every 3rd element\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(res, 'szse_sme')\n",
    "                res.clear()\n",
    "        except:\n",
    "            print('error', cur_date)\n",
    "            continue\n",
    "    save_to_file(res, 'szse_sme')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start = date(2016, 9, 13)\n",
    "    end = date(2016, 9, 21)\n",
    "    \n",
    "    czce(date_range(start, end))\n",
    "    shfe(date_range(start, end))\n",
    "    dce(date_range(start, end))\n",
    "    cffex(date_range(start, end))\n",
    "    szse_main(date_range(start, end))\n",
    "    szse_sme(date_range(start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# czce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different link after 20151006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from grab import Grab\n",
    "from datetime import timedelta, date\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: generator with business days\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def find_links(dates):\n",
    "    \"\"\"\n",
    "    create links\n",
    "    :param dates: days range when we need to parse data\n",
    "    :return: list with links\n",
    "    \"\"\"\n",
    "    _links = []\n",
    "    # old link format\n",
    "    # url = 'http://english.czce.com.cn/enportal/exchange/marketdata/{0}/datadaily/{1}.htm'\n",
    "    # new link format from 20151008\n",
    "    url = 'http://english.czce.com.cn/enportal/DFSStaticFiles/Future/{0}/{1}/EnglishFutureDataDaily.htm'\n",
    "    for single_date in dates:\n",
    "        _links.append(url.format(single_date.strftime(\"%Y\"), single_date.strftime(\"%Y%m%d\")))\n",
    "    return _links\n",
    "\n",
    "\n",
    "def parse_data(linkz):\n",
    "    \"\"\"\n",
    "    parse data from each link\n",
    "    :param linkz: list with links\n",
    "    :return: list with data\n",
    "    \"\"\"\n",
    "    g = Grab()\n",
    "    _data = []\n",
    "    for l in linkz:\n",
    "        try:\n",
    "            g.go(l)\n",
    "            for i in g.doc.select('//*[@id=\"senfe\"]/tr'):\n",
    "                if 'Total' in i.text():\n",
    "                    # old format\n",
    "                    # _data.append((l[-12:-4], i.text().split(' ')[1], i.text().split(' ')[2]))\n",
    "                    # print('added', l[-12:-4])\n",
    "                    # new format\n",
    "                    _data.append((l[-35:-27], i.text().split(' ')[1], i.text().split(' ')[2]))\n",
    "                    print('added', l[-35:-27])\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "        except:\n",
    "            print('error', l[-12:-4])\n",
    "            continue\n",
    "    return _data\n",
    "\n",
    "\n",
    "def save_to_file(data_save):\n",
    "    with open(\"czce_new.txt\", \"w\") as f:\n",
    "        f.write(\"(Date, Total_Volume, Total_Open_Interest)\")\n",
    "        f.write('\\n')\n",
    "        for raw in data_save:\n",
    "            f.write(str(raw))\n",
    "            f.write('\\n')\n",
    "    print(\"Saved to file {}\".format(f.name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = date(2015, 10, 8)\n",
    "    end = date(2016, 8, 21)\n",
    "    links = find_links(date_range(start, end))\n",
    "    data = parse_data(links)\n",
    "    save_to_file(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "import time\n",
    "from random import randint\n",
    "import requests\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: generator with business days\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def find_links(dates):\n",
    "    \"\"\"\n",
    "    create links\n",
    "    :param dates: days range when we need to parse data\n",
    "    :return: list with links\n",
    "    \"\"\"\n",
    "    _links = []\n",
    "    url = 'http://www.shfe.com.cn/data/dailydata/kx/kx{}.dat'\n",
    "    for single_date in dates:\n",
    "        _links.append(url.format(single_date.strftime(\"%Y%m%d\")))\n",
    "    return _links\n",
    "\n",
    "\n",
    "def parse_data(linkz):\n",
    "    \"\"\"\n",
    "    parse data from each link\n",
    "    :param linkz: list with links\n",
    "    :return: list with data\n",
    "    \"\"\"\n",
    "    _data = []\n",
    "    for i, l in enumerate(linkz):\n",
    "        try:\n",
    "            r = requests.get(l).json()\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "            _data.append((l[-12:-4], r['o_curinstrument'][-1]['VOLUME'],\n",
    "                         r['o_curinstrument'][-1]['OPENINTEREST']))\n",
    "            print('added', l[-12:-4])\n",
    "            if i % 3 == 0:\n",
    "                save_to_file(_data)\n",
    "                _data.clear()\n",
    "        except:\n",
    "            print('error', l[-12:-4])\n",
    "            continue\n",
    "    save_to_file(_data)\n",
    "    return _data\n",
    "\n",
    "\n",
    "def save_to_file(data_save):\n",
    "    with open(\"shfe.txt\", \"a\") as f:\n",
    "        #f.write(\"(Date, Total_Volume, Total_Open_Interest)\")\n",
    "        #f.write('\\n')\n",
    "        for raw in data_save:\n",
    "            f.write(str(raw))\n",
    "            f.write('\\n')\n",
    "    print(\"Saved to file {}\".format(f.name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = date(2016, 4, 11)\n",
    "    end = date(2016, 8, 22)\n",
    "    links = find_links(date_range(start, end))\n",
    "    data = parse_data(links)\n",
    "    #save_to_file(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from grab import Grab\n",
    "from datetime import timedelta, date\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: generator with business days\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def grab_data(dates):\n",
    "    g = Grab()\n",
    "    url = 'http://www.dce.com.cn/PublicWeb/MainServlet'\n",
    "\n",
    "    res = []\n",
    "    for indx, date in enumerate(dates):\n",
    "        try:\n",
    "            # post request\n",
    "            cur_date = int(date.strftime(\"%Y%m%d\"))\n",
    "            _data = {'action': 'Pu00231_result',\n",
    "                'Pu00231_Input.trade_date': cur_date,\n",
    "                'Pu00231_Input.variety': 'all',\n",
    "                'Pu00231_Input.trade_type': 0,\n",
    "                'Submit': 'Go'}\n",
    "            g.go(url, post=_data)\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "            # find 'total' element and grab its Volume and OI\n",
    "            for i in g.doc.select('//body/table/tr/td/*/tr'):\n",
    "                if 'Total' in i.text():\n",
    "                    d = i.text().split(' ')\n",
    "                    res.append((cur_date, d[1], d[2]))\n",
    "            print('added', cur_date)\n",
    "            # save all, every 3rd element\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(res)\n",
    "                res.clear()\n",
    "        except:\n",
    "            print('error', cur_date)\n",
    "            continue\n",
    "    save_to_file(res)\n",
    "\n",
    "    \n",
    "def save_to_file(data_save):\n",
    "    with open(\"dce.txt\", \"a\") as f:\n",
    "        #f.write(\"(Date, Total_Volume, Total_Open_Interest)\")\n",
    "        #f.write('\\n')\n",
    "        for raw in data_save:\n",
    "            f.write(str(raw))\n",
    "            f.write('\\n')\n",
    "    print(\"Saved to file {}\".format(f.name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = date(2016, 8, 15)\n",
    "    end = date(2016, 8, 21)\n",
    "    grab_data(date_range(start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cffex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from grab import Grab\n",
    "from datetime import timedelta, date\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: generator with business days\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def save_to_file(data_save):\n",
    "    with open(\"cffex.txt\", \"a\") as f:\n",
    "        # f.write(\"(Date, Total_Volume, Total_Open_Interest)\")\n",
    "        # f.write('\\n')\n",
    "        for raw in data_save:\n",
    "            f.write(str(raw))\n",
    "            f.write('\\n')\n",
    "    print(\"Saved to file {}\".format(f.name))\n",
    "\n",
    "\n",
    "def create_links(dates):\n",
    "    url = 'http://www.cffex.com.cn/fzjy/mrhq/{0}/{1}/index.xml'\n",
    "    _links = []\n",
    "\n",
    "    for date in dates:\n",
    "        _links.append(url.format(date.strftime(\"%Y%m\"), date.strftime(\"%d\")))\n",
    "    return _links\n",
    "\n",
    "\n",
    "def grab_data(links):\n",
    "    g = Grab()\n",
    "    _data = []\n",
    "\n",
    "    for indx, l in enumerate(links):\n",
    "        try:\n",
    "            g.go(l)\n",
    "            tree = g.doc.build_xml_tree()\n",
    "\n",
    "            daily_vol = []\n",
    "            daily_oi = []\n",
    "\n",
    "            for i in tree:\n",
    "                for j in i.iter():\n",
    "                    if j.tag == 'volume':\n",
    "                        # print(j.tag, ' = ', j.text)\n",
    "                        vol = j.text\n",
    "                        daily_vol.append(int(vol.split('.')[0]))\n",
    "                    elif j.tag == 'openinterest':\n",
    "                        # print(j.tag, ' = ', j.text)\n",
    "                        oi = j.text\n",
    "                        daily_oi.append(int(oi.split('.')[0]))\n",
    "                        # print(sum(daily_vol), sum(daily_oi))\n",
    "            _data.append((l[-19:-13] + l[-12:-10], sum(daily_vol), sum(daily_oi)))\n",
    "            daily_vol.clear()\n",
    "            daily_oi.clear()\n",
    "            print('added: ', _data[-1])\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(_data)\n",
    "                _data.clear()\n",
    "        except:\n",
    "            print('error ', l[-19:-13] + l[-12:-10])\n",
    "            continue\n",
    "    save_to_file(_data)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = date(2014, 8, 17)\n",
    "    end = date(2016, 8, 22)\n",
    "    all_links = create_links(date_range(start, end))\n",
    "    grab_data(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# szse main_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from grab import Grab\n",
    "from datetime import timedelta, date\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: generator with business days\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def grab_data(dates):\n",
    "    g = Grab()\n",
    "    url = 'http://www.szse.cn/szseWeb/FrontController.szse?randnum=0.06895916919770584'\n",
    "\n",
    "    res = []\n",
    "    for indx, date in enumerate(dates):\n",
    "        try:\n",
    "            # post request\n",
    "            cur_date = date.strftime(\"%Y-%m-%d\")\n",
    "            post_data = {'ACTIONID':7,\n",
    "                        'AJAX':'AJAX-TRUE',\n",
    "                        'CATALOGID':1849,\n",
    "                        'txtQueryDate':cur_date,\n",
    "                        'TABKEY':'tab2'}\n",
    "            g.go(url, post=post_data)\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "            # find 'shares traded' element and grab its Volume\n",
    "            volume = []\n",
    "            for tr in g.doc.select('//table[@class=\"cls-data-table\"]/tr'):\n",
    "                if 'Shares' in tr.text():\n",
    "                    for td in tr.select('td'):\n",
    "                        volume.append(td.text())\n",
    "            print(cur_date,volume[1])\n",
    "            res.append((cur_date, volume[1]))\n",
    "            # save all, every 3rd element\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(res)\n",
    "                res.clear()\n",
    "        except:\n",
    "            print('error', cur_date)\n",
    "            continue\n",
    "    save_to_file(res)\n",
    "\n",
    "    \n",
    "def save_to_file(data_save):\n",
    "    with open(\"szse_main.txt\", \"a\") as f:\n",
    "        for raw in data_save:\n",
    "            f.write(str(raw))\n",
    "            f.write('\\n')\n",
    "    print(\"Saved to file {}\".format(f.name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = date(2014, 8, 15)\n",
    "    end = date(2016, 8, 24)\n",
    "    grab_data(date_range(start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# szse SME board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from grab import Grab\n",
    "from datetime import timedelta, date\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: generator with business days\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def grab_data(dates):\n",
    "    g = Grab()\n",
    "    url = 'http://www.szse.cn/szseWeb/FrontController.szse?randnum=0.06895916919770584'\n",
    "\n",
    "    res = []\n",
    "    for indx, date in enumerate(dates):\n",
    "        try:\n",
    "            # post request\n",
    "            cur_date = date.strftime(\"%Y-%m-%d\")\n",
    "            post_data = {'ACTIONID':7,\n",
    "                        'AJAX':'AJAX-TRUE',\n",
    "                        'CATALOGID':1849,\n",
    "                        'txtQueryDate':cur_date,\n",
    "                        'TABKEY':'tab3'}\n",
    "            g.go(url, post=post_data)\n",
    "            time.sleep(randint(0, 3))  # try to avoid ban\n",
    "            # find 'shares traded' element and grab its Volume\n",
    "            volume = []\n",
    "            for tr in g.doc.select('//table[@class=\"cls-data-table\"]/tr'):\n",
    "                if 'Shares' in tr.text():\n",
    "                    for td in tr.select('td'):\n",
    "                        volume.append(td.text())\n",
    "            print(cur_date,volume[1])\n",
    "            res.append((cur_date, volume[1]))\n",
    "            # save all, every 3rd element\n",
    "            if indx % 3 == 0:\n",
    "                save_to_file(res)\n",
    "                res.clear()\n",
    "        except:\n",
    "            print('error', cur_date)\n",
    "            continue\n",
    "    save_to_file(res)\n",
    "\n",
    "    \n",
    "def save_to_file(data_save):\n",
    "    with open(\"szse_sme.txt\", \"a\") as f:\n",
    "        for raw in data_save:\n",
    "            f.write(str(raw))\n",
    "            f.write('\\n')\n",
    "    print(\"Saved to file {}\".format(f.name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = date(2014, 8, 15)\n",
    "    end = date(2016, 8, 24)\n",
    "    grab_data(date_range(start, end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}